# -*- coding: utf-8 -*-
"""Baseline_English.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jWl3_4MP1P6LEUNiYUuZivtrc016QQ6y
"""

import tensorflow as tf
print(tf.test.gpu_device_name())
# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth
# config = tf.ConfigProto()
# config.gpu_options.allow_growth = True

from keras.preprocessing.text import Tokenizer
from gensim.models.fasttext import FastText
import numpy as np
import nltk
import pandas as pd
from nltk.tokenize import word_tokenize

!pip install fasttext

import fasttext as ft

data = pd.read_csv('train_tweet.csv')
data.head()

from google.colab import drive
drive.mount('/content/drive')

data = data.dropna()
tweets = data['text'].tolist()

for i in range(len(tweets)):
  tweets[i]=tweets[i].lower()

len(tweets)

data['label'] = data['label'].apply(lambda x : 1 if x == 4 else x)

one_hot = pd.get_dummies(data['label'],dtype=float)

labels = one_hot.values

type(labels)

labels = list(labels)
for l in range(len(labels)):
  labels[l] = list(labels[l])

type(labels[0])

target_word_embeddings = []

model = ft.load_model('cc.en.300.bin')

for sent in tweets:
  fast_vec = model.get_sentence_vector(sent)
  target_word_embeddings.append(fast_vec)

len(target_word_embeddings[0])

len(labels[0])

from keras import Model
from keras.layers import Input, Dense, Bidirectional
from keras.layers.recurrent import LSTM
from keras.models import Sequential

# from sklearn.model_selection import train_test_split
# train_texts, test_texts, train_labels, test_labels = train_test_split(target_word_embeddings, labels, test_size=0)

import tensorflow as tf

tf.config.run_functions_eagerly(True)

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

# def define_model():
#     input1 = Input(shape=(1,300)) #take the reshape last two values, see "data = np.reshape(data,(10,2,1))" which is "data/batch-size, row, column"
#     lstm1 = Bidirectional(LSTM(units=32),return_sequences=True)(input1)
#     #lstm1 = tf.keras.layers.LSTM(4)
#     dnn_hidden_layer1 = Dense(3, activation='relu')(lstm1)
#     dnn_hidden_layer2 = Dense(3, activation='relu')(lstm1)
#     dnn_output = Dense(2, activation='softmax')(dnn_hidden_layer1)
#     model = Model(inputs=[input1],outputs=[dnn_output])
#     # compile the model
#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
#     model.summary()
#     return model

import keras

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall
 
def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision
 
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

model = keras.Sequential()
model.add(Bidirectional(LSTM(128, return_sequences=True,
               input_shape=(1, 300))))  # input layer
model.add(LSTM(128, return_sequences=True))  # layer 1
model.add(LSTM(128, return_sequences=True))  # layer 2
model.add(LSTM(128, return_sequences=True))  # layer 3
model.add(LSTM(128))  # layer 4
model.add(Dense(2, activation='sigmoid')) # output layer
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc',f1_m,precision_m, recall_m])
#model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

input_shape = (None,1,300)
model.build(input_shape)
model.summary()

target_word_embeddings = np.reshape(target_word_embeddings,(len(target_word_embeddings),1,300))

len(target_word_embeddings[0])

target_word_embeddings.shape

train_labels = np.array(train_labels)

labels.shape

labels = np.array(labels)

with tf.device('/gpu:0'): model.fit(target_word_embeddings,labels,validation_split=0.1,epochs=10,batch_size=32,verbose=1)

model.save('/content/drive/MyDrive/baseline_english')