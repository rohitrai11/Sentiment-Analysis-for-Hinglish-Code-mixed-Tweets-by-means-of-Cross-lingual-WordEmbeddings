# -*- coding: utf-8 -*-
"""Experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lgCJlw0Q0pmsv1Y_xR2RGrS9SCo3eURb
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install datasets

import pandas as pd
import torch

data = pd.read_csv('train.csv')
data['tweet'] = data['tweet'].astype(str)
data.head()

data['sentiment'] = data['sentiment'].apply(lambda x : 2 if x == -1 else x)
data['tweet'] = data['tweet'].apply(lambda t: t.lower())
data.head()

# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# MbertTokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
# MbertModel = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# MbertModel.classifier.out_features = 3

# MbertModel

from transformers import AutoTokenizer, AutoModelForSequenceClassification
MbertTokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
MbertModel = AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-cased",num_labels=3,output_hidden_states=True)

"""#### Tokenize the data"""

data = data.dropna()
text = data['tweet'].tolist()

seq_len = [len(i.split()) for i in text]

pd.Series(seq_len).hist(bins = 30)

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(text, data['sentiment'].tolist(), test_size=.01)

train_encodings = MbertTokenizer(train_texts, add_special_tokens=True,padding=True,truncation=True,max_length=20)
val_encodings = MbertTokenizer(val_texts, add_special_tokens=True,padding=True,truncation=True,max_length=20)

train_encodings[0]

count = 0
for para in MbertModel.parameters():
  count += 1
print(f'Total model parameters:{count}')

from torch import nn
import copy

def deleteEncodingLayers(model, layers_to_throw):  # must pass in the full bert model
    oldModuleList = model.bert.encoder.layer
    newModuleList = nn.ModuleList()

    # Now iterate over all layers, only keepign only the relevant layers.
    for i in range(0, len(oldModuleList)):
        if i not in layers_to_throw:
          newModuleList.append(oldModuleList[i])

    # create a copy of the model, modify it with the new list, and return
    copyOfModel = copy.deepcopy(model)
    copyOfModel.bert.encoder.layer = newModuleList

    return copyOfModel

#MbertModel_new = deleteEncodingLayers(MbertModel,[5])

param_list = list(MbertModel.named_parameters())
print(len(param_list))

# exempt = [j for j in range(len(param_list)-8,len(param_list))]
# param_count = -1
# for param in MbertModel.parameters():
#   param_count += 1
#   if param_count not in exempt:
#     param.requires_grad = False

# for param in MbertModel.parameters():
#   print(param.requires_grad)

"""#### Convert data to Pytorch Dataset Object"""

class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
      item={}
      for key,val in self.encodings.items():
        item[key] = torch.tensor(val[idx])
        # if key != 'token_type_ids':
        #   item[key] = torch.tensor(val[idx])
        item['labels'] = torch.tensor(self.labels[idx])
       
      return item

    def __len__(self):
        return len(self.labels)

train_dataset = Dataset(train_encodings,train_labels)
val_dataset = Dataset(val_encodings,val_labels)

#define metrics
import numpy as np
from datasets import load_metric
from transformers import Trainer
metric = load_metric('accuracy')

#define training hyperparameters
from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir='/drive/MyDrive/train_args2',
    evaluation_strategy='epoch',
    save_strategy='epoch',
    num_train_epochs=8,
    learning_rate=5e-6,
    per_device_train_batch_size=30,
    per_device_eval_batch_size=30,
    load_best_model_at_end=True,
    save_total_limit=2
     
    )

#define metric computation
def compute_metrics(eval_pred):
  #print(eval_pred[0][0].shape)
  #print(eval_pred[0])
  logits = eval_pred[0][0]
  labels = eval_pred[1]
  #logits = eval
  predictions = np.argmax(logits,axis=-1)
  return metric.compute(predictions=predictions,references=labels)

#define trainer object
trainer = Trainer(
    model = MbertModel,
    args = training_args,
    train_dataset = train_dataset,
    eval_dataset = val_dataset,
    compute_metrics = compute_metrics,
)

trainer.train()

one_hot = pd.get_dummies(data['sentiment'],dtype=float)
labels = one_hot.values
labels = list(labels)

for l in range(len(labels)):
  labels[l] = list(labels[l])

target_word_embeddings = []
device = torch.device("cuda")

#MbertModel

for txt in text:
  # Add the special tokens.
  # c=c+1
  marked_text = "[CLS] " + txt + " [SEP]"

  # Split the sentence into tokens.
  tokenized_text = MbertTokenizer.tokenize(marked_text)

  # Map the token strings to their vocabulary indeces.
  indexed_tokens = MbertTokenizer.convert_tokens_to_ids(tokenized_text)

  # Convert inputs to PyTorch tensors
  tokens_tensor = torch.tensor([indexed_tokens],device=device)

  # Put the model in "evaluation" mode, meaning feed-forward operation.
  MbertModel.eval()

  with torch.no_grad():

    outputs = MbertModel(tokens_tensor)

    hidden_states = outputs[1]

    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0)

    target_word_embeddings.append(word_embed_5[0][0])

for i in range(len(target_word_embeddings)):
  target_word_embeddings[i] = target_word_embeddings[i].tolist()

len(target_word_embeddings)

train_texts = target_word_embeddings
train_labels = labels

from keras import Model
from keras.layers import Input, Dense, Bidirectional
from keras.layers.recurrent import LSTM

import tensorflow as tf
tf.config.run_functions_eagerly(True)

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

def define_model():
    input1 = Input(shape=(1,768)) #take the reshape last two values, see "data = np.reshape(data,(10,2,1))" which is "data/batch-size, row, column"
    lstm1 = Bidirectional(LSTM(units=32))(input1)
    dnn_hidden_layer1 = Dense(3, activation='relu')(lstm1)
    dnn_output = Dense(3, activation='softmax')(dnn_hidden_layer1)
    model = Model(inputs=[input1],outputs=[dnn_output])
    # compile the model
    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])
    model.summary()
    return model

# from tensorflow import keras

# model = keras.Sequential()
# model.add(Bidirectional(LSTM(32, return_sequences=True,
#                input_shape=(1, 768))))  # returns a sequence of vectors of dimension 32
# model.add(Bidirectional(LSTM(32, return_sequences=True)))  # returns a sequence of vectors of dimension 32
# model.add(Bidirectional(LSTM(32)))  # return a single vector of dimension 32
# model.add(Dense(3, activation='softmax'))
# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc',f1_m,precision_m, recall_m])

train_texts = np.reshape(train_texts,(len(train_texts),1,768))

LSTM_model = define_model()
#LSTM_model = model

train_labels = np.array(train_labels)

LSTM_model.fit(train_texts,train_labels,validation_split=0.1,epochs=4,batch_size=10,verbose=1)

#Save Model
LSTM_model.save('/drive/MyDrive/fine_tuned_lstm_model')

"""##Testing"""

data = pd.read_csv('test.csv')
data['tweet'] = data['tweet'].astype(str)
data.head()

data['sentiment'] = data['sentiment'].apply(lambda x : 2 if x == -1 else x)
data['tweet'] = data['tweet'].apply(lambda t: t.lower())
data.head()

for i in range(len(data['sentiment'])):
  if data['sentiment'][i] == 'positive':
    data['sentiment'][i] = 1
  elif data['sentiment'][i] == 'negative':
    data['sentiment'][i] = 2
  else:
    data['sentiment'][i] = 0

data.head(5)

data = data.dropna()
text = data['tweet'].tolist()

test_texts = text
test_labels = data['sentiment'].tolist()

one_hot = pd.get_dummies(data['sentiment'],dtype=float)
test_labels = one_hot.values
test_labels = list(test_labels)

for l in range(len(test_labels)):
  test_labels[l] = list(test_labels[l])

target_word_embeddings = []
device = torch.device("cuda")

for txt in test_texts:
  # Add the special tokens.
  # c=c+1
  marked_text = "[CLS] " + txt + " [SEP]"

  # Split the sentence into tokens.
  tokenized_text = MbertTokenizer.tokenize(marked_text)

  # Map the token strings to their vocabulary indeces.
  indexed_tokens = MbertTokenizer.convert_tokens_to_ids(tokenized_text)

  # Convert inputs to PyTorch tensors
  tokens_tensor = torch.tensor([indexed_tokens],device=device)

  # Put the model in "evaluation" mode, meaning feed-forward operation.
  MbertModel.eval()

  with torch.no_grad():

    outputs = MbertModel(tokens_tensor)

    hidden_states = outputs[1]

    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0)

    target_word_embeddings.append(word_embed_5[0][0])

for i in range(len(target_word_embeddings)):
  target_word_embeddings[i] = target_word_embeddings[i].tolist()

test_texts = target_word_embeddings
Xtest = np.reshape(test_texts,(len(test_texts),1,768))
ytest = np.array(test_labels)

# evaluate the model
loss,accuracy,precision, recall = LSTM_model.evaluate(Xtest, ytest, verbose=0)

from sklearn import metrics
 
predictions  =LSTM_model.predict(Xtest)

y_test = np.argmax(np.array(ytest),axis=1)
y_pred = np.argmax(predictions,axis=1)
 
print(metrics.confusion_matrix(y_test, y_pred))
 
# Print the precision and recall, among other metrics
print(metrics.classification_report(y_test, y_pred, digits=3))