# -*- coding: utf-8 -*-
"""Baseline_Hinglish_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e6jPc3rTynRwXuCq-BA2KYah5SrrbJyN
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
print(tf.test.gpu_device_name())
# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth
# config = tf.ConfigProto()
# config.gpu_options.allow_growth = True

from keras.preprocessing.text import Tokenizer
from gensim.models.fasttext import FastText
import numpy as np
import nltk
import pandas as pd
from nltk.tokenize import word_tokenize

!pip install fasttext

import fasttext as ft

data = pd.read_csv('train.csv')
data.head()

data = data.dropna()
tweets = data['tweet'].tolist()

for i in range(len(tweets)):
  tweets[i]=tweets[i].lower()

len(tweets)

data['sentiment'] = data['sentiment'].apply(lambda x : 2 if x == -1 else x)

one_hot = pd.get_dummies(data['sentiment'],dtype=float)

labels = one_hot.values

type(labels)

labels = list(labels)
for l in range(len(labels)):
  labels[l] = list(labels[l])

type(labels[0])

target_word_embeddings = []

#f_model = ft.load_model('/content/drive/MyDrive/cc.en.300.bin')

from gensim.utils import tokenize
import pandas as pd


def create_data():
  text = ""
  data = pd.read_csv('train.csv',encoding='utf-8')
  data = data.dropna()
  for twt in data['tweet']:
    text = text + twt
    #text.append(twt)
  with open('data.txt', 'w') as f:
    f.write(text)

create_data()

# f_model = FastText(vector_size=300, window=4, min_count=0)

# f_model.build_vocab(MyIter())
# total_examples = f_model.corpus_count
# f_model.train(MyIter(), total_examples=total_examples, epochs=20)

f_model = ft.train_unsupervised('data.txt', model='cbow',dim=300)

for sent in tweets:
  fast_vec = f_model.get_sentence_vector(sent)
  target_word_embeddings.append(fast_vec)

len(target_word_embeddings[0])

len(labels[0])

from keras import Model
from keras.layers import Input, Dense, Bidirectional
from keras.layers.recurrent import LSTM
from keras.models import Sequential

# from sklearn.model_selection import train_test_split
# train_texts, test_texts, train_labels, test_labels = train_test_split(target_word_embeddings, labels, test_size=0)

import tensorflow as tf

tf.config.run_functions_eagerly(True)

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

# def define_model():
#     input1 = Input(shape=(1,300)) #take the reshape last two values, see "data = np.reshape(data,(10,2,1))" which is "data/batch-size, row, column"
#     lstm1 = Bidirectional(LSTM(units=32),return_sequences=True)(input1)
#     #lstm1 = tf.keras.layers.LSTM(4)
#     dnn_hidden_layer1 = Dense(3, activation='relu')(lstm1)
#     dnn_hidden_layer2 = Dense(3, activation='relu')(lstm1)
#     dnn_output = Dense(2, activation='softmax')(dnn_hidden_layer1)
#     model = Model(inputs=[input1],outputs=[dnn_output])
#     # compile the model
#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
#     model.summary()
#     return model

import keras

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall
 
def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision
 
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

model = keras.Sequential()
model.add(Bidirectional(LSTM(128, return_sequences=True,
               input_shape=(1, 300))))  # input layer
model.add(LSTM(128, return_sequences=True))  # layer 1
model.add(LSTM(128, return_sequences=True))  # layer 2
model.add(LSTM(128, return_sequences=True))  # layer 3
model.add(LSTM(128))  # layer 4
model.add(Dense(3, activation='softmax')) # output layer
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc',f1_m,precision_m, recall_m])
#model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

input_shape = (None,1,300)
model.build(input_shape)
model.summary()

len(target_word_embeddings[0])

target_word_embeddings = np.reshape(target_word_embeddings,(len(target_word_embeddings),1,300))

len(target_word_embeddings[0])

target_word_embeddings.shape

#train_labels = np.array(train_labels)

labels = np.array(labels)

labels.shape

with tf.device('/gpu:0'): model.fit(target_word_embeddings,labels,validation_split=0.1,epochs=20,batch_size=16,verbose=1)

#model.save('/content/drive/MyDrive/baseline_hinglish_final')

"""##Test"""

data = pd.read_csv('test.csv')
data.head()

data = data.dropna()
tweets = data['tweet'].tolist()

for i in range(len(tweets)):
  tweets[i]=tweets[i].lower()

data['sentiment'] = data['sentiment'].apply(lambda x : 2 if x == -1 else x)

one_hot = pd.get_dummies(data['sentiment'],dtype=float)

test_labels = one_hot.values

test_labels = list(test_labels)
for l in range(len(test_labels)):
  test_labels[l] = list(test_labels[l])

test_word_embeddings = []
for sent in tweets:
  fast_vec = f_model.get_sentence_vector(sent)
  test_word_embeddings.append(fast_vec)


test_word_embeddings = np.reshape(test_word_embeddings,(len(test_word_embeddings),1,300))

from sklearn import metrics
 
predictions = model.predict(target_word_embeddings)

y_test = np.argmax(np.array(labels),axis=1)
y_pred = np.argmax(predictions,axis=1)
 
print(metrics.confusion_matrix(y_test, y_pred))
 
# Print the precision and recall, among other metrics
print(metrics.classification_report(y_test, y_pred, digits=3))

