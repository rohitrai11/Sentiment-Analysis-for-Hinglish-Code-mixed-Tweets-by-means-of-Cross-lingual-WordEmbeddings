# -*- coding: utf-8 -*-
"""MBERT+BiLSTM_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8sjhGEksdyVumDtw3pPJpTdyzAP6TFM
"""

pip freeze > requirements.txt

!pip install numpy
!pip install torch
!pip install sklearn
!pip install pytorch_transformers

!pip install transformers

# from pytorch_transformers import BertTokenizer
# from pytorch_transformers import BertModel

# ## Load pretrained model/tokenizer
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-cased",output_hidden_states = True)

import pandas as pd
import numpy as np
import nltk
import torch

data = pd.read_csv('train.csv')
data.head()

data['sentiment'] = data['sentiment'].apply(lambda x : 2 if x == -1 else x)

# one_hot = pd.get_dummies(data['sentiment'])

data = data.dropna()
tweets = data['tweet'].tolist()

for i in range(len(tweets)):
  tweets[i]=tweets[i].lower()

len(tweets)

one_hot = pd.get_dummies(data['sentiment'],dtype=float)

labels = one_hot.values

type(labels)

labels = list(labels)

for l in range(len(labels)):
  labels[l] = list(labels[l])

type(labels[0])

device = torch.device("cuda")

model.to(device)

model.device

target_word_embeddings = []

for text in tweets:
  # Add the special tokens.
  # c=c+1
  marked_text = "[CLS] " + text + " [SEP]"

  # Split the sentence into tokens.
  tokenized_text = tokenizer.tokenize(marked_text)

  # Map the token strings to their vocabulary indeces.
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

  # Convert inputs to PyTorch tensors
  tokens_tensor = torch.tensor([indexed_tokens],device=device)

  # Put the model in "evaluation" mode, meaning feed-forward operation.
  model.eval()

  with torch.no_grad():

    outputs = model(tokens_tensor)

    hidden_states = outputs[1]

    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0)

    target_word_embeddings.append(word_embed_5[0][0])

    #print(c,"   ",len(word_embed_5[0][0]))

for i in range(len(target_word_embeddings)):
  target_word_embeddings[i] = target_word_embeddings[i].tolist()

len(target_word_embeddings)

from keras import Model
from keras.layers import Input, Dense, Bidirectional
from keras.layers.recurrent import LSTM
from keras.models import Sequential

# from sklearn.model_selection import train_test_split
# train_texts, test_texts, train_labels, test_labels = train_test_split(target_word_embeddings, labels, test_size=.1)

#len(train_texts)

#type(train_labels[0])

import tensorflow as tf

tf.config.run_functions_eagerly(True)

#len(train_texts[0])

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

import keras

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall
 
def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision
 
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

Bi_LSTM_model = keras.Sequential()
Bi_LSTM_model.add(Bidirectional(LSTM(128, return_sequences=True,
               input_shape=(1, 768))))  # input layer
Bi_LSTM_model.add(LSTM(128, return_sequences=True))  # layer 1
Bi_LSTM_model.add(LSTM(128, return_sequences=True))  # layer 2
Bi_LSTM_model.add(LSTM(128, return_sequences=True))  # layer 3
Bi_LSTM_model.add(LSTM(128))  # layer 4
Bi_LSTM_model.add(Dense(3, activation='softmax')) # output layer
Bi_LSTM_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc',f1_m,precision_m, recall_m])
#model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

input_shape = (None,1,768)
Bi_LSTM_model.build(input_shape)
Bi_LSTM_model.summary()

target_word_embeddings = np.reshape(target_word_embeddings,(len(target_word_embeddings),1,768))

len(target_word_embeddings[0])

target_word_embeddings.shape

labels = np.array(labels)

labels.shape

with tf.device('/gpu:0'): Bi_LSTM_model.fit(target_word_embeddings,labels,validation_split=0.1,epochs=30,batch_size=16,verbose=1)

# from google.colab import drive
# drive.mount('/content/drive')

#Bi_LSTM_model.save('/content/drive/MyDrive/mbert_bilstm_model')

"""##Testing Model##"""

test_data = pd.read_csv('test.csv')
#data.head()


test_data['sentiment'] = test_data['sentiment'].apply(lambda x : 2 if x == -1 else x)


test_data = test_data.dropna()
tweets = test_data['tweet'].tolist()


for i in range(len(tweets)):
  tweets[i]=tweets[i].lower()


one_hot = pd.get_dummies(test_data['sentiment'],dtype=float)


labels_test = one_hot.values

labels_test = list(labels_test)

for l in range(len(labels_test)):
  labels_test[l] = list(labels_test[l])



test_word_embeddings = []


for text in tweets:
  # Add the special tokens.
  # c=c+1
  marked_text = "[CLS] " + text + " [SEP]"

  # Split the sentence into tokens.
  tokenized_text = tokenizer.tokenize(marked_text)

  # Map the token strings to their vocabulary indeces.
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

  # Convert inputs to PyTorch tensors
  tokens_tensor = torch.tensor([indexed_tokens],device=device)

  # Put the model in "evaluation" mode, meaning feed-forward operation.
  model.eval()

  with torch.no_grad():

    outputs = model(tokens_tensor)

    hidden_states = outputs[1]

    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0)

    test_word_embeddings.append(word_embed_5[0][0])

    #print(c,"   ",len(word_embed_5[0][0]))





for i in range(len(test_word_embeddings)):
  test_word_embeddings[i] = test_word_embeddings[i].tolist()

test_word_embeddings = np.reshape(test_word_embeddings,(len(test_word_embeddings),1,768))

predictions = Bi_LSTM_model.predict(test_word_embeddings)

print(len(labels_test))
print(np.array(labels_test).shape)
print(predictions.shape)

from sklearn import metrics

y_test = np.argmax(np.array(labels_test),axis=1)
y_pred = np.argmax(predictions,axis=1)

print(metrics.confusion_matrix(y_test, y_pred))

# Print the precision and recall, among other metrics
print(metrics.classification_report(y_test, y_pred, digits=3))

# def define_model():
#     input1 = Input(shape=(1,768)) #take the reshape last two values, see "data = np.reshape(data,(10,2,1))" which is "data/batch-size, row, column"
#     lstm1 = Bidirectional(LSTM(units=32))(input1)
#     dnn_hidden_layer1 = Dense(3, activation='relu')(lstm1)
#     dnn_output = Dense(3, activation='softmax')(dnn_hidden_layer1)
#     model = Model(inputs=[input1],outputs=[dnn_output])
#     # compile the model
#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
#     model.summary()
#     return model

#train_texts = np.reshape(train_texts,(len(train_texts),1,768))

#len(train_texts[0])

#train_texts.shape

#len(train_texts[0])

#LSTM_model = define_model()

# LSTM_model.to(device)

#train_labels = np.array(train_labels)

#train_labels.shape

#type(train_labels[0])

#LSTM_model.fit(train_texts,train_labels,epochs=15,batch_size=10,verbose=1)

# target_word_embeddings[6117].size()

# target_word_embeddings[0]

# len(word_embed_5[0][0])

tweets[0]

# import tensorflow as tf

# from tensorflow.keras import layers

# from tensorflow.keras.layers import Input, Lambda, Bidirectional, Dense, Dropout

# from tensorflow.keras.models import Model

# for i in range(len(target_word_embeddings)):
#   target_word_embeddings[i]=target_word_embeddings[i].tolist()

# BiLSTM = Bidirectional(layers.LSTM(1024, return_sequences= False, recurrent_dropout=0.2, dropout=0.2), name="BiLSTM")(target_word_embeddings)